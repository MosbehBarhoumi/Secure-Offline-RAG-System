import os
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.text_splitter import CharacterTextSplitter
from langchain.llms import Ollama
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.document_loaders import TextLoader

# Load and process the text file
def load_and_process_text(file_path):
    loader = TextLoader(file_path)
    documents = loader.load()
    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
    texts = text_splitter.split_documents(documents)
    return texts

# Initialize embeddings and vector store
def initialize_vector_store(texts):
    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    vectorstore = FAISS.from_documents(texts, embeddings)
    return vectorstore

# Initialize the Ollama model
def initialize_llm():
    return Ollama(model="qwen2.5:3b")

# Set up the conversational chain
def setup_chain(vectorstore, llm):
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
    chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vectorstore.as_retriever(),
        memory=memory
    )
    return chain

# Chat interface
def chat_interface(chain):
    print("Welcome to the RAG Chat Interface!")
    print("Type 'exit' to end the conversation.")
    
    chat_history = []
    while True:
        query = input("You: ")
        if query.lower() == 'exit':
            break
        
        result = chain({"question": query, "chat_history": chat_history})
        print("AI:", result['answer'])
        
        chat_history.append((query, result['answer']))

# Main function
def main():
    # Specify the path to your text file
    file_path = "context.txt"  # Replace with your actual file path
    
    # Load and process the text
    texts = load_and_process_text(file_path)
    
    # Initialize vector store
    vectorstore = initialize_vector_store(texts)
    
    # Initialize LLM
    llm = initialize_llm()
    
    # Set up the chain
    chain = setup_chain(vectorstore, llm)
    
    # Start the chat interface
    chat_interface(chain)

if __name__ == "__main__":
    main()